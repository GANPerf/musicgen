{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from audiocraft.models import MusicGen\n",
    "from audiocraft.data.audio import audio_write\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "model = MusicGen.get_pretrained('small')\n",
    "    \n",
    "from audiocraft.modules.conditioners import (\n",
    "    ClassifierFreeGuidanceDropout\n",
    ")\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm = model.lm.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = {\n",
    "    1: {\n",
    "        'audio': '/home/ubuntu/dataset/segment_000.wav',\n",
    "        'label': '/home/ubuntu/dataset/segment_000.txt'\n",
    "    },\n",
    "    2: {\n",
    "        'audio': '/home/ubuntu/dataset/segment_001.wav',\n",
    "        'label': '/home/ubuntu/dataset/segment_001.txt'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.00001\n",
    "model.lm.train()\n",
    "optimizer = AdamW(model.lm.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def count_nans(tensor):\n",
    "    nan_mask = torch.isnan(tensor)\n",
    "    num_nans = torch.sum(nan_mask).item()\n",
    "    return num_nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(audio_path, model: MusicGen, duration: int = 30):\n",
    "    wav, sr = torchaudio.load(audio_path)\n",
    "    wav = torchaudio.functional.resample(wav, sr, model.sample_rate)\n",
    "    wav = wav.mean(dim=0, keepdim=True)\n",
    "    end_sample = int(model.sample_rate * duration)\n",
    "    wav = wav[:, :end_sample]\n",
    "\n",
    "    assert wav.shape[0] == 1\n",
    "    assert wav.shape[1] == model.sample_rate * duration\n",
    "\n",
    "    wav = wav.cuda()\n",
    "    wav = wav.unsqueeze(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen_audio = model.compression_model.encode(wav)\n",
    "\n",
    "    codes, scale = gen_audio\n",
    "\n",
    "    assert scale is None\n",
    "\n",
    "    return codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: /home/ubuntu/dataset/segment_000.txt\n",
      "audio: /home/ubuntu/dataset/segment_000.wav\n",
      "audio shape: torch.Size([1, 4, 1500])\n",
      "audio: tensor[1, 4, 1500] i64 n=6000 (47Kb) x∈[0, 2047] μ=1.149e+03 σ=622.125 cuda:0\n"
     ]
    }
   ],
   "source": [
    "text = pairs[1]['label']\n",
    "audio = pairs[1]['audio']\n",
    "# print both\n",
    "print(\"text:\", text)\n",
    "print(\"audio:\", audio)\n",
    "\n",
    "audio = preprocess_audio(audio, model)\n",
    "# print audio info\n",
    "print(\"audio shape:\", audio.shape)\n",
    "print(\"audio:\", audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attributes: [ConditioningAttributes(text={'description': 'funky song by moeshop, electropop, hype, fast paced, webcore'}, wav={'self_wav': WavCondition(wav=tensor[1, 1] cuda:0 [[0.]], length=tensor[1] i64 cuda:0 [0], path='null_wav')})]\n"
     ]
    }
   ],
   "source": [
    "text = open(text, 'r').read().strip()\n",
    "\n",
    "attributes, _ = model._prepare_tokens_and_attributes([text], None)\n",
    "# print attributes info\n",
    "print(\"attributes:\", attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null_conditions: [ConditioningAttributes(text={'description': None}, wav={'self_wav': WavCondition(wav=tensor[1, 1] cuda:0 [[0.]], length=tensor[1] i64 cuda:0 [0], path=['null_wav'])})] \n",
      "\n",
      "conditions [ConditioningAttributes(text={'description': 'funky song by moeshop, electropop, hype, fast paced, webcore'}, wav={'self_wav': WavCondition(wav=tensor[1, 1] cuda:0 [[0.]], length=tensor[1] i64 cuda:0 [0], path='null_wav')}), ConditioningAttributes(text={'description': None}, wav={'self_wav': WavCondition(wav=tensor[1, 1] cuda:0 [[0.]], length=tensor[1] i64 cuda:0 [0], path=['null_wav'])})] \n",
      "\n",
      "tokenized {'description': {'input_ids': tensor[2, 15] i64 n=30 x∈[0, 18789] μ=1.930e+03 σ=4.392e+03 cuda:0, 'attention_mask': tensor[2, 15] i64 n=30 x∈[0, 1] μ=0.500 σ=0.509 cuda:0}} \n",
      "\n",
      "Cfg {'description': (tensor[2, 15, 1024] n=30720 (0.1Mb) x∈[-8.109, 8.683] μ=0.002 σ=0.389 grad MulBackward0 cuda:0, tensor[2, 15] i64 n=30 x∈[0, 1] μ=0.500 σ=0.509 cuda:0)} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "conditions = attributes\n",
    "null_conditions = ClassifierFreeGuidanceDropout(p=1.0)(conditions)\n",
    "print(\"null_conditions:\", null_conditions, '\\n')\n",
    "conditions = conditions + null_conditions\n",
    "print(\"conditions\", conditions, '\\n')\n",
    "tokenized = model.lm.condition_provider.tokenize(conditions)\n",
    "print(\"tokenized\", tokenized, '\\n')\n",
    "cfg_conditions = model.lm.condition_provider(tokenized)\n",
    "print(\"Cfg\", cfg_conditions, '\\n')\n",
    "condition_tensors = cfg_conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def one_hot_encode(tensor, num_classes=6):\n",
    "    shape = tensor.shape\n",
    "    one_hot = torch.zeros((shape[0], shape[1], num_classes))\n",
    "\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            index = tensor[i, j].item()\n",
    "            one_hot[i, j, index] = 1\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 1500])\n",
      "tensor[2, 4, 1500] i64 n=12000 (94Kb) x∈[0, 2047] μ=1.149e+03 σ=622.099 cuda:0\n"
     ]
    }
   ],
   "source": [
    "codes = torch.cat([audio, audio], dim=0)\n",
    "print(codes.shape)\n",
    "print(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm = model.lm.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model.autocast:\n",
    "    # 3. Pass encoded_audio and text_embeddings to compute_predictions()\n",
    "    lm_output = model.lm.compute_predictions(\n",
    "        codes=codes,\n",
    "        conditions=[],\n",
    "        condition_tensors=condition_tensors\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = codes[0]\n",
    "logits = lm_output.logits[0]\n",
    "mask = lm_output.mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = one_hot_encode(codes, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1500, 2048])\n",
      "torch.Size([4, 1500, 2048])\n",
      "torch.Size([6000])\n"
     ]
    }
   ],
   "source": [
    "print(codes.shape)\n",
    "print(logits.shape)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten mask tensor\n",
    "mask = mask.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_logits = logits.view(-1, 2048)[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[5994, 2048] f16 n=12275712 (23Mb) x∈[-26.906, 25.359] μ=-3.502 σ=4.516 grad IndexBackward0 cuda:0\n",
       "tensor([[-4.9976e-01, -2.7988e+00, -4.4922e+00,  ..., -1.4417e-01,\n",
       "         -3.6641e+00, -4.1719e+00],\n",
       "        [ 1.6807e+00, -4.3477e+00, -4.0234e+00,  ...,  1.0430e+00,\n",
       "         -1.9404e+00, -3.5020e+00],\n",
       "        [ 8.9600e-02, -9.5312e+00, -4.5508e+00,  ...,  2.0137e+00,\n",
       "         -3.5762e+00, -5.8594e+00],\n",
       "        ...,\n",
       "        [-1.4229e+00,  3.6836e+00, -7.8760e-01,  ...,  2.8149e-01,\n",
       "         -3.6797e+00,  2.0488e+00],\n",
       "        [-1.6533e+00,  6.6797e-01, -6.7529e-01,  ..., -4.2227e+00,\n",
       "         -5.5389e-03, -7.9834e-01],\n",
       "        [-3.8535e+00,  7.5703e+00, -9.3115e-01,  ...,  2.3789e+00,\n",
       "          3.7480e+00,  5.0898e+00]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_logits.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = codes.cpu()\n",
    "mask = mask.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_codes = codes.view(-1, 2048)[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.chans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_logits = masked_logits.cuda()\n",
    "masked_codes = masked_codes.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(masked_logits, masked_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor grad DivBackward1 cuda:0 2.968\n",
       "tensor(2.9675, device='cuda:0', grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor cuda:0 21.087"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.utils.clip_grad_norm_(model.lm.parameters(), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
